%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[manuscript,screen,review]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2021}
\acmYear{2021}
\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[KDD '212]{Woodstock '18: ACM Symposium on Neural
  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,
  Aug 14--18, 2021, Singapore}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{FairGAMEe: unsupervised bias detection in black box models learned without demographics}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Brian Barr}
%\authornote{Both authors contributed equally to this research.}
\email{brian.barr@capitalone.com}
%\orcid{1234-5678-9012}

%\author{G.K.M. Tobin}
%\authornotemark[1]
%\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Center for Machine Learning}
  \streetaddress{11 W 19th}
  \city{New York}
  \state{New York}
  \country{USA}
  \postcode{10011}
}

% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \streetaddress{Rono-Hills}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \streetaddress{30 Shuangqing Rd}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \streetaddress{8600 Datapoint Drive}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}
%   \postcode{78229}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Barr, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Fairness in model predictions, as well as being an implicit ethical requirement, can be enforced by laws  generically over the whole population (e.g. GPDR) or by regulation on industries (e.g. fair lending laws).  A challenge arises when the model developer is tasked with building a fair model without demographic information (i.e. with all protected class information being withheld).  We propose a post-hoc analysis method called \textit{FairGAMe} to detect the presence of bias in the model using model centric subgroups formed from clusters of model explanations.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010293.10003660</concept_id>
<concept_desc>Computing methodologies~Classification and regression trees</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010258</concept_id>
<concept_desc>Computing methodologies~Learning paradigms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010258.10010260.10003697</concept_id>
<concept_desc>Computing methodologies~Cluster analysis</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Classification and regression trees}
\ccsdesc[500]{Computing methodologies~Learning paradigms}
\ccsdesc[500]{Computing methodologies~Cluster analysis}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{neural networks, explainability, fairness, clustering}


  \begin{teaserfigure}
    \includegraphics[width=\textwidth]{example-image-a}
    \caption{teaser image caption}
    \Description{figure description}
  \end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Predictions from machine learning models have become common place in every day life, in a vast array of scenarios - from recommending products and movies, to finding a date, to automating decisions on loans, to predicting the risk a criminal will commit another crime.

To help explain the predictions made from these models, explainable artificial intelligence (XAI) has provided a host of methods to show which of a model's features has the highest importance relating to that individual data sample (e.g. saliency maps, SHAP, deep lift, LIME, and counterfactual explanations).  The explanations of predictions can insure that the model is making the predictions for the right reason.

However, this by itself is not sufficient when the predictions have an impact on humans.  It is possible that the data used to train the model carries historical bias against segments of the population - e.g. the fact that historically women make less than men, see~\citet{dalessandroConscientiousClassificationData2017} for an overview.  A model trained on such data can cause that historical bias to become entrenched and perpetuate into the future, even in the face of changing public opinion.  To combat this, statistical measures of fairness have been developed.  Model predictions aggregated over segments of the population can be compared to other segments to gauge a model's fairness.

That sort of statistical assessment of a model's fairness is not always possible - the demographics of the data samples may be missing, or not shared with the model developers (see e.g. fairness through unawareness).  Methods have been proposed to increase a model's fairness through various re-weighting of the training samples.  What seems to be lacking in this space of fairness metrics and attempts to fix a model - is a method to detect the possibility of bias.  If it ain't broke, don't fix it.

We propose a method of detecting model bias when it is trained on data lacking demographic features using unsupervised clustering on model explanations - a process we call FairGAMe.  Through a series of experiments using synthetic data and black box models, we provide detection of model bias through its performance on machine learning metrics (like AUC) as well as on suitable (problem specific) fairness metrics calculated on the subpopulations determined by the clustering algorithm.  We also provide intuitive statistical insights as to the cause of a model's bias.

\section{Previous work}
This work sits at the intersection of machine learning, explainability and fairness. 
XAI

Fairness Metrics

Model adjustments - actions to be taken

precision medicine paper (same approach - different topic).  

This line of work is nice in that it highlights that models can sometimes learn additional useful signal that is revealed in the analysis of the explanations.

Our contribution is
\begin{itemize}
    \item a bias\/fairness testing framework using synthetic data
    \item a novel method for detection of bias in models trained without demographic information
    \item insights on the statistical causes of model bias
\end{itemize}

The analysis should:
\begin{itemize}
    \item describe what's contained in each cluster - important variables, segments of marginal distributions of input variables, possibly tie back to demographics (which is cheating since in the real world this cannot be done)
    \item compare and contrast behavior of fair vs unfair data
\end{itemize}


\section{Synthetic datasets}
\subsection{Problem formulation}
Imagine a data scientist working at a company, interested in predicting if they will get a promotion during the current employee evaluation cycle.  Leveraging their network of friends, they anonymously collect data from previous years on features (and their type): productivity (categorical), salary (continuous), bonus (continuous), years of service (continuous),  and performance rating (categorical).  In our hypothetical story, consider in an alternate universe, they also collect data on gender (binary) and race (categorical) - the data for all other features being identical. In both universes, the problem is formulated as a binary classification problem and a model is fit.  Given the historical data, the model has enough predictive capability to be useful.

Some colleagues get wind of this side of desk project - and do predictions on their own data.  Some are unhappy with the outcomes - thinking that they had deserved a promotion previously and did not get one.   After hours conversations start raising questions if there are patterns of missed opportunities that are more widespread than just at the individual level.

Our interest lies in the universe that did not collect demographic information since it is the most challenging (and hence interesting from a data science perspective).  At times however, it will prove useful to glimpse at the demographic data from the other universe.


\subsection{Synthetic bias}
Synthetic bias can be injected into the classification labels is of two types:
\begin{itemize}
    \item overt - when terms in our symbolic expression depend on the value of the protected attributes - gender and race.
    \item proxied - when other features are correlated with protected attributes.  This allows for leakage of information about the protected attributes, even if they are not explicitly used as features during model training.
\end{itemize}


\subsection{Statistical considerations of the input variables}
WHI workshop library - added multinomial categorical variables



\subsection{Model form}


\subsection{Explanations}

\subsection{Decisions}

\subsection{Fairness metric}
Cite Verma and Rubin - overview of possible metrics.

We will adopt a four fifths rule on FNR as our fairness metric.  A false negative is denying someone a promotion that would be worthy.  This metric will be applied not only to the gender and race categories, but also the intersectional groups formed by combining gender and race to investigate biased interactions in the data.

\section{Experiments}

\subsection{Fair data analysis}

\subsection{other examples}
Other experiments to run to verify/solidify this hypothesis:

\begin{itemize}
    \item overt
    \item proxied
    \item Balanced race $[\frac{1}{3}, \frac{1}{3}, \frac{1}{3}] $
    \item Imbalanced promotions (i.e. imbalanced labels)
    \item Separate data generation processes for each race ( Across g and r?)
\end{itemize}


\section{Conclusions}
We have 
\begin{itemize}
    \item established a principled method of experimentation on algorithmic bias 
    \item demonstrated a unique unsupervised bias detection algorithm based on clustering of model explanations.
    \item explored the impact of overt and proxied synthetic bias
    \item shown the difference between this detection and computational identifiability (stretch)
    \item shown how sample weights can ensure recovery (as opposed to amplification) of bias that exists in the data. (stretch)
    \item without demographics - it's unclear how you would create fair data...this is future work?  (at least more lit search...)
\end{itemize}



\section{CCS Concepts and User-Defined Keywords}

Two elements of the ``acmart'' document class provide powerful
taxonomic tools for you to help readers find your work in an online
search.

The ACM Computing Classification System ---
\url{https://www.acm.org/publications/class-2012} --- is a set of
classifiers and concepts that describe the computing
discipline. Authors can select entries from this classification
system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
commands to be included in the \LaTeX\ source.

User-defined keywords are a comma-separated list of words and phrases
of the authors' choosing, providing a more flexible way of describing
the research being presented.

CCS concepts and user-defined keywords are required for for all
articles over two pages in length, and are optional for one- and
two-page articles (or abstracts).



\begin{table}
  \caption{Frequency of Special Characters}
  \label{tab:freq}
  \begin{tabular}{ccl}
    \toprule
    Non-English or Math&Frequency&Comments\\
    \midrule
    \O & 1 in 1,000& For Swedish names\\
    $\pi$ & 1 in 5& Common in math\\
    \$ & 4 in 5 & Used in business\\
    $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
  \bottomrule
\end{tabular}
\end{table}

To set a wider table, which takes up the whole width of the page's
live area, use the environment \textbf{table*} to enclose the table's
contents and the table caption.  As with a single-column table, this
wide table will ``float'' to a location deemed more
desirable. Immediately following this sentence is the point at which
Table~\ref{tab:commands} is included in the input file; again, it is
instructive to compare the placement of the table here with the table
in the printed output of this document.

\begin{table*}
  \caption{Some Typical Commands}
  \label{tab:commands}
  \begin{tabular}{ccl}
    \toprule
    Command &A Number & Comments\\
    \midrule
    \texttt{{\char'134}author} & 100& Author \\
    \texttt{{\char'134}table}& 300 & For tables\\
    \texttt{{\char'134}table*}& 400& For wider tables\\
    \bottomrule
  \end{tabular}
\end{table*}



\subsection{The ``Teaser Figure''}

A ``teaser figure'' is an image, or set of images in one figure, that
are placed after all author and affiliation information, and before
the body of the article, spanning the page. If you wish to have such a
figure in your article, place the command immediately before the
\verb|\maketitle| command:
\begin{verbatim}
  \begin{teaserfigure}
    \includegraphics[width=\textwidth]{sampleteaser}
    \caption{figure caption}
    \Description{figure description}
  \end{teaserfigure}
\end{verbatim}

\section{Citations and Bibliographies}

The use of \BibTeX\ for the preparation and formatting of one's
references is strongly recommended. Authors' names should be complete
--- use full first names (``Donald E. Knuth'') not initials
(``D. E. Knuth'') --- and the salient identifying features of a
reference should be included: title, year, volume, number, pages,
article DOI, etc.

The bibliography is included in your source document with these two
commands, placed just before the \verb|\end{document}| command:
\begin{verbatim}
  \bibliographystyle{ACM-Reference-Format}
  \bibliography{bibfile}
\end{verbatim}
where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|''
suffix, of the \BibTeX\ file.

Citations and references are numbered by default. A small number of
ACM publications have citations and references formatted in the
``author year'' style; for these exceptions, please include this
command in the {\bfseries preamble} (before the command
``\verb|\begin{document}|'') of your \LaTeX\ source:
\begin{verbatim}
  \citestyle{acmauthoryear}
\end{verbatim}


\section{Appendices}

If your work needs an appendix, add it before the
``\verb|\end{document}|'' command at the conclusion of your source
document.

Start the appendix with the ``\verb|appendix|'' command:
\begin{verbatim}
  \appendix
\end{verbatim}
and note that in the appendix, sections are lettered, not
numbered. This document has two appendices, demonstrating the section
and subsection identification method.


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
The author would like to thank a bunch of people for making this paper become a reality.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{fairgame}

%%
%% If your work has an appendix, this is the place to put it.
\appendix

\section{Research Methods}

\subsection{Part One}


\section{Online Resources}


\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
